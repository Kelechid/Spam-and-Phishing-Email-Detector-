{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1qjuRkpOQ-HAJVuldyJ3325-V_E7z44OD","authorship_tag":"ABX9TyPZmUe+SmnTipCIF84yPMPZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v82r7IikCHdS","executionInfo":{"status":"ok","timestamp":1746290759924,"user_tz":-180,"elapsed":14114,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"267f104c-00bd-44b7-ca42-5ee78c9ba8c9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import string\n","import pandas as pd\n","import numpy as np\n","import re\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout"],"metadata":{"id":"YZDOKBTzCUED"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EYmCJrP9xdl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746378047109,"user_tz":-180,"elapsed":1966,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"61bce8f1-aa1f-4148-e463-e2203bbf8537"},"outputs":[{"output_type":"stream","name":"stdout","text":["Final cleaned + merged dataset shape: (11300, 2)\n","                                                    text  spam\n","11295  Yes i have. So that's why u texted. Pshew...mi...     0\n","11296  the national forum on corporate finance  mr . ...     0\n","11297  why johan dahl and the mri energy staffing gro...     0\n","11298  perfect visual solution for your business now ...     1\n","11299  Do u konw waht is rael FRIENDSHIP Im gving yuo...     0\n"]}],"source":["# Load datasets\n","emails_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/emails.csv')\n","email_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/email.csv')\n","\n","#rename and map labels from email file\n","email_df.rename(columns={'Message': 'text', 'Category': 'spam'}, inplace=True)\n","email_df['spam'] = email_df['spam'].map({'spam': 1, 'ham': 0})\n","\n","# Remove any empty rows or rows with missing values\n","email_df = email_df[email_df['spam'].notna()]\n","\n","# Remove \"Subject:\" from the beginning of the emails\n","emails_df['text'] = emails_df['text'].str.replace(r'^Subject:\\s*', '', regex=True).str.strip()\n","\n","# Merge datasets\n","merged_df = pd.concat([emails_df[['text', 'spam']], email_df[['text', 'spam']]], ignore_index=True)\n","\n","# Convert spam column to integers\n","merged_df['spam'] = merged_df['spam'].astype(int)\n","\n","#merge and shuffle the dataset to mix the two data from the different files\n","merged_df = merged_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","print(f\"Final cleaned + merged dataset shape: {merged_df.shape}\")\n","print(merged_df.tail())\n"]},{"cell_type":"code","source":["merged_df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekG7Jh4DRm6d","executionInfo":{"status":"ok","timestamp":1746378053219,"user_tz":-180,"elapsed":21,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"20e357ac-4376-4372-aacb-8fbff1429a73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 11300 entries, 0 to 11299\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype \n","---  ------  --------------  ----- \n"," 0   text    11300 non-null  object\n"," 1   spam    11300 non-null  int64 \n","dtypes: int64(1), object(1)\n","memory usage: 176.7+ KB\n"]}]},{"cell_type":"markdown","source":["#Preprocessing\n"],"metadata":{"id":"QEjd_d5dItKd"}},{"cell_type":"code","source":["#download the nltk package\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"crYb2FROPmeu","executionInfo":{"status":"ok","timestamp":1746378059025,"user_tz":-180,"elapsed":79,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"c1187dae-0814-468f-9579-3f534381abd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Initialize stemmer and stopwords\n","stemmer = PorterStemmer()\n","stopwords_set = set(stopwords.words('english'))\n","\n","cleaned = []\n","\n","#The for loop would go through each word and lowercase, remove html tags,\n","#remove line breaks, urls, numbers,punctuattiion and apply stemmer and check for stopwords before adding it to the list\n","for i in range(len(merged_df)):\n","    text = merged_df['text'].iloc[i].lower()\n","    text = re.sub(r'<.*?>', '', text)                 # Remove HTML tags\n","    text = re.sub(r'\\n+', ' ', text)                  # Remove line breaks\n","    text = re.sub(r'https?://\\S+', '', text)          # Remove URLs\n","    text = re.sub(r'\\d+', '', text)                   # Remove numbers\n","    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n","    tokens = text.split()\n","    tokens = [stemmer.stem(word) for word in tokens if word not in stopwords_set]\n","    cleaned_text = ' '.join(tokens)\n","    cleaned.append(cleaned_text)"],"metadata":{"id":"Fv1LukoJRflX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_df.text.iloc[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"WPsysyS1Tc4T","executionInfo":{"status":"ok","timestamp":1746378083058,"user_tz":-180,"elapsed":39,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"d158912f-8ff9-4733-8afb-4947f6e493da"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'year end 2000 performance feedback  note : you will receive this message each time you are selected as a reviewer .  you have been selected to participate in the year end 2000 performance  management process by providing meaningful feedback on specific employee ( s ) .  your feedback plays an important role in the process , and your participation  is critical to the success of enron \\' s performance management goals .  to complete requests for feedback , access pep at http : / / pep . corp . enron . com  and select perform review under performance review services . you may begin  providing feedback immediately and are requested to have all feedback forms  completed by friday , november 17 , 2000 .  if you have any questions regarding pep or your responsibility in the  process , please contact the pep help desk at :  houston : 1 . 713 . 853 . 4777 , option 4  london : 44 . 207 . 783 . 4040 , option 4  email : perfmgmt @ enron . com  thank you for your participation in this important process .  the following is a cumulative list of employee feedback requests with a  status of \" open . \" once you have submitted or declined an employee \\' s request  for feedback , their name will no longer appear on this list .  review group : enron  feedback due date : nov 17 , 2000  employee name supervisor name date selected  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -  crenshaw , shirley j wincenty j kaminski oct 26 , 2000  kindall , kevin vasant shanbhogue oct 30 , 2000  lamas vieira pinto , rodrigo david port oct 31 , 2000  supatgiat , chonawee peyton s gibner oct 27 , 2000  tamarchenko , tanya v vasant shanbhogue oct 26 , 2000  villarreal , norma e sheila h walton oct 26 , 2000  walton , sheila h david oxley oct 27 , 2000  yaman , sevil vasant shanbhogue oct 27 , 2000  yuan , ding richard l carson oct 31 , 2000'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["cleaned[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"JBLJPohNTZgR","executionInfo":{"status":"ok","timestamp":1746378086251,"user_tz":-180,"elapsed":392,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"1e9543d1-13bf-4541-98d0-4b69606e3209"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'year end perform feedback note receiv messag time select review select particip year end perform manag process provid meaning feedback specif employe feedback play import role process particip critic success enron perform manag goal complet request feedback access pep http pep corp enron com select perform review perform review servic may begin provid feedback immedi request feedback form complet friday novemb question regard pep respons process pleas contact pep help desk houston option london option email perfmgmt enron com thank particip import process follow cumul list employe feedback request statu open submit declin employe request feedback name longer appear list review group enron feedback due date nov employe name supervisor name date select crenshaw shirley j wincenti j kaminski oct kindal kevin vasant shanbhogu oct lama vieira pinto rodrigo david port oct supatgiat chonawe peyton gibner oct tamarchenko tanya v vasant shanbhogu oct villarr norma e sheila h walton oct walton sheila h david oxley oct yaman sevil vasant shanbhogu oct yuan ding richard l carson oct'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["#Apply Vectorization with TfidfVectorizer\n","vectorizer = TfidfVectorizer(max_features=5000)\n","x = vectorizer.fit_transform(cleaned).toarray()\n","y = merged_df['spam'].astype(int)"],"metadata":{"id":"kyKBPOFCTqu5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Train Model\n"],"metadata":{"id":"MNAUQtV7M9zu"}},{"cell_type":"code","source":["#Normalize input features (Bag of Words)\n","scaler = StandardScaler()\n","x_normalized = scaler.fit_transform(x)\n","\n","\n","#split off test set (20%)\n","x_temp, x_test, y_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","#remaining 80% into 70% train and 10% validation\n","x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.125, random_state=42)  # 0.125 * 0.8 = 0.1\n","\n","print(f\"Train shape: {x_train.shape}, Val shape: {x_val.shape}, Test shape: {x_test.shape}\")\n","\n","#Train the Deep neural network\n","\n","model = Sequential([\n","    Dense(512, activation='relu', input_shape=(x.shape[1],)),\n","    Dropout(0.3),\n","    Dense(128, activation='relu'),\n","    Dropout(0.2),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","history = model.fit(x_train, y_train, epochs=10, batch_size=32,\n","                    validation_data=(x_val, y_val))\n","\n","test_loss, test_acc = model.evaluate(x_test, y_test)\n","print(f\"\\n Test Accuracy: {test_acc:.4f}\")\n","\n","#Classification report\n","y_pred_probs = model.predict(x_test)\n","y_pred = (y_pred_probs > 0.5).astype(int)\n","\n","print(\"\\n Classification Report:\")\n","print(classification_report(y_test, y_pred, digits=4))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzPa_aoBnPfK","executionInfo":{"status":"ok","timestamp":1746378117859,"user_tz":-180,"elapsed":21665,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"06ccb93d-9bfb-44e9-894e-d54e5db2d011"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train shape: (7910, 5000), Val shape: (1130, 5000), Test shape: (2260, 5000)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - accuracy: 0.8690 - loss: 0.3108 - val_accuracy: 0.9752 - val_loss: 0.0772\n","Epoch 2/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0329 - val_accuracy: 0.9770 - val_loss: 0.0790\n","Epoch 3/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9955 - loss: 0.0152 - val_accuracy: 0.9779 - val_loss: 0.0888\n","Epoch 4/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9984 - loss: 0.0074 - val_accuracy: 0.9743 - val_loss: 0.0975\n","Epoch 5/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0060 - val_accuracy: 0.9735 - val_loss: 0.1089\n","Epoch 6/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0054 - val_accuracy: 0.9761 - val_loss: 0.1182\n","Epoch 7/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9997 - loss: 0.0022 - val_accuracy: 0.9752 - val_loss: 0.1227\n","Epoch 8/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9991 - loss: 0.0033 - val_accuracy: 0.9735 - val_loss: 0.1301\n","Epoch 9/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9985 - loss: 0.0043 - val_accuracy: 0.9717 - val_loss: 0.1354\n","Epoch 10/10\n","\u001b[1m248/248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9994 - loss: 0.0027 - val_accuracy: 0.9717 - val_loss: 0.1396\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9748 - loss: 0.1331\n","\n"," Test Accuracy: 0.9765\n","\u001b[1m71/71\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n","\n"," Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0     0.9818    0.9897    0.9857      1849\n","           1     0.9520    0.9173    0.9343       411\n","\n","    accuracy                         0.9765      2260\n","   macro avg     0.9669    0.9535    0.9600      2260\n","weighted avg     0.9764    0.9765    0.9764      2260\n","\n"]}]},{"cell_type":"markdown","source":["#REAL LIFE APPLICATION"],"metadata":{"id":"l9bIr8OApaom"}},{"cell_type":"code","source":["import joblib\n","\n","joblib.dump(vectorizer, 'vectorizer.pkl')\n","joblib.dump(scaler, 'scaler.pkl')\n","model.save('spam_classifier_model.h5')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1M2fBsDpfl-","executionInfo":{"status":"ok","timestamp":1746302622664,"user_tz":-180,"elapsed":159,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"fb119970-a356-4acd-ddf4-19dd21ac6c1c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"]}]},{"cell_type":"code","source":["import joblib\n","from tensorflow.keras.models import load_model\n","from nltk.stem.porter import PorterStemmer\n","from nltk.corpus import stopwords\n","import string\n","import numpy as np\n","import re\n","\n","# Load saved files\n","vectorizer = joblib.load('vectorizer.pkl')\n","scaler = joblib.load('scaler.pkl')\n","model = load_model('spam_classifier_model.h5')\n","\n","# Text preprocessing function\n","stemmer = PorterStemmer()\n","stopwords_set = set(stopwords.words('english'))\n","\n","def preprocess(text):\n","    text = text.lower()\n","    text = re.sub(r'<.*?>', '', text)  # remove HTML tags\n","    text = re.sub(r'\\n+', ' ', text)   # remove line breaks\n","    text = text.translate(str.maketrans('', '', string.punctuation)).split()\n","    text = [stemmer.stem(word) for word in text if word not in stopwords_set]\n","    return ' '.join(text)\n","\n","#  Paste email here:\n","sample_email = \"\"\"  \"\"\"\n","# Preprocess\n","processed = preprocess(sample_email)\n","vectorized = vectorizer.transform([processed]).toarray()\n","normalized = scaler.transform(vectorized)\n","\n","# Predict\n","prediction = model.predict(normalized)\n","\n","# Output\n","if prediction[0][0] > 0.5:\n","    print(\"🟥 SPAM\")\n","else:\n","    print(\"🟩 NOT SPAM\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qa_yrEOZpitu","executionInfo":{"status":"ok","timestamp":1746302782533,"user_tz":-180,"elapsed":492,"user":{"displayName":"Kelechi David","userId":"11849599861675787153"}},"outputId":"fae8f754-145c-4f74-f0ca-3197b28374c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n","🟩 NOT SPAM\n"]}]}]}